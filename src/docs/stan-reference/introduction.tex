\part{Introduction}


\chapter{Overview}

\noindent
This document is both a user's guide and a reference manual for
\Stan's probabilistic modeling language.  This introductory chapter
provides a high-level overview of \Stan.  The next chapter provides a
hands-on quick-start guide showing how \Stan works in practice.
Installation instructions are in \refappendix{install}. The remaining
parts of this document include a practically-oriented user's guide for
programming models and a detailed reference manual for \Stan's
modeling language and associated programs and data formats.

\section{\Stan Programs}

A \Stan program defines a statistical model through a conditional
probability function $p(\theta|y;x)$, where $\theta$ is a sequence of
modeled unknown values (e.g., model parameters, latent variables, missing
data, future predictions), $y$ is a sequence of modeled known 
values, and $x$ is a sequence of unmodeled predictors and constants
(e.g., sizes, hyperparameters).

\Stan programs consist of variable type declarations and statements.
Variable types include constrained and unconstrained integer, scalar,
vector, and matrix types, as well as (multidimensional) arrays of
other types.  Variables are declared in blocks corresponding to the
variable's use: data, transformed data, parameter, transformed
parameter, or generated quantity.  Unconstrained local variables may
be declared within statement blocks.

Statements in \Stan are interpreted imperatively, so their order
matters.  Atomic statements involve the assignment of a value to a
variable.  Sequences of statements (and optionally local variable
declarations) may be organized into a block.  \Stan also provides bounded
for-each loops of the sort used in \R and \BUGS.

The transformed data, transformed parameter, and generated quantities
blocks contain statements defining the variables declared in their
blocks.  A special model block consists of statements defining the log
probability for the model.

Within the model block, \BUGS-style sampling notation may be used as
shorthand for incrementing an underlying log probability variable, the
value of which defines the log probability function.  The log
probability variable may also be accessed directly, allowing
user-defined probability functions and Jacobians of transforms.


\section{Compiling and Running \Stan Programs}

A \Stan program is first compiled to a \Cpp program by the \Stan
compiler \stanc, then the \Cpp program compiled to a self-contained
platform-specific executable.  \Stan can generate executables for
various flavors of Windows, Mac OS X, and Linux.%
%
\footnote{A \Stan program may also be compiled to a dynamically
  linkable object file for use in a higher-level scripting language
  such as \R or Python.}
%
Running the \Stan executable for a model first reads in and validates
the known values $y$ and $x$, then generates a sequence of
(non-independent) identically distributed samples $\theta^{(1)},
\theta^{(2)}, \ldots$, each of which has the marginal distribution
$p(\theta|y;x)$.


\section{\Stan's Samplers}

For continuous parameters, \Stan uses Hamiltonian Monte Carlo (\HMC)
sampling \citep{Duane:1987, Neal:1994, Neal:2011}, a form of Markov chain Monte
Carlo (\MCMC) sampling \citep{Metropolis:1953}.  \Stan 1.0 can only sample discrete parameters
on which no other parameters depend, such as simulated values.
\refchapter{mixture-modeling} discusses how finite discrete parameters
can be summed out of models.

\HMC accelerates both convergence to the stationary distribution and
subsequent parameter exploration by using the gradient of the log
probability function.  The unknown quantity vector $\theta$ is
interpreted as the position of a fictional particle.  Each iteration
generates a random momentum and simulates the path of the particle
with potential energy determined the (negative) log probability
function.  Hamilton's decomposition shows that the gradient of this
potential determines change in momentum and the momentum determines
the change in position.  These continuous changes over time are
approximated using the leapfrog algorithm, which breaks the time into
discrete steps which are easily simulated.  A Metropolis reject step
is then applied to correct for any simulation error and ensure
detailed balance of the resulting Markov chain transitions
\citep{Metropolis:1953, Hastings:1970}.

Standard \HMC involves three ``tuning'' parameters to which its
behavior is quite sensitive.  \Stan's samplers allow these parameters
to be set by hand or set automatically without user intervention.

The first tuning parameter is a mass matrix for the fictional
particle.  \Stan can be configured to use a unit mass matrix or to
estimate a diagonal mass matrix during warmup; in the future, it will
also support a user-defined diagonal mass matrix.  Estimating the mass
matrix normalizes the scale of each element $\theta_k$ of the unknown
variable sequence $\theta$.

The other two tuning parameters set the temporal step size of the
discretization of the Hamiltonian and the total number of steps taken
per iteration.  \Stan can be configured with a user-specified step
size or it can estimate an optimal step size during warmup using dual
averaging \citep{Nesterov:2009, Hoffman-Gelman:2012}.  In either case, additional
randomization may be applied to draw the step size from an interval of
possible step sizes \citep{Neal:2011}.

\Stan can be set to use a specified number of steps, or it can
automatically adapt the number of steps during sampling using the
no-U-turn (\NUTS) sampler \citep{Hoffman-Gelman:2012}.  


\section{Convergence Monitoring and Effective Sample Size}

Samples in a Markov chain are only drawn with the marginal
distribution $p(\theta|y;x)$ after the chain has converged to its
equilibrium distribution.  There are several methods to test whether
an \MCMC method has failed to converge; unfortunately, passing the
tests does not guarantee convergence.  The recommended method for
\Stan is to run multiple Markov chains each with different diffuse
initial parameter values, discard the warmup/adaptation samples, then
split the remainder of each chain in half and compute the potential
scale reduction statistic, $\hat{R}$ \citep{GelmanRubin:1992}.

When estimating a mean based on $M$ independent samples, the
estimation error is proportional to $1/\sqrt{M}$.  If the samples are
positively correlated, as they typically are when drawn using \MCMC
methods, the error is proportional to $1/\sqrt{\mbox{\sc ess}}$, where
{\sc ess} is the effective sample size.  Thus it is standard practice
to also monitor (an estimate of) the effective sample size of
parameters of interest in order to estimate the additional estimation
error due to correlated samples.





\section{Bayesian Inference and Monte Carlo Methods}

\Stan was developed to support full Bayesian inference.  Bayesian
inference is based in part on Bayes's rule,
\[
p(\theta|y;x) \propto p(y|\theta;x) \, p(\theta;x),
\]
which, in this unnormalized form, states that the posterior
probability $p(\theta|y;x)$ of parameters $\theta$ given data $y$ (and
constants $x$) is proportional (for fixed $y$ and $x$) to the
product of the likelihood function $p(y|\theta;x)$ and prior
$p(\theta;x)$.

For \Stan, Bayesian modeling involves coding the posterior probability
function up to a proportion, which Bayes's rule shows is equivalent to
modeling the product of the likelihood function and prior up to a
proportion.

Full Bayesian inference involves propagating the uncertainty in the
value of parameters $\theta$ modeled by the posterior $p(\theta|y;x)$.
This can be accomplished by basing inference on a sequence of samples
from the posterior using plug-in estimates for quantities of interest
such as posterior means, posterior intervals, predictions based on the
posterior such as event outcomes or the values of as yet unobserved
data.



\chapter{Getting Started}

\noindent
This chapter is designed to help users get acquainted with the overall
design of the \Stan language and calling \Stan from the command line.
Later chapters are devoted to expanding on the material in this
chapter with full reference documentation.  The content is identical
to that found on the getting-started with the command-line
documentation on the Stan home page, \url{http://mc-stan.org/}.

\section{For BUGS Users}

\refappendix{stan-for-bugs} describes some similarities
and important differences between Stan and BUGS (including WinBUGS,
OpenBUGs, and JAGS).


\section{Installation}

For information about supported versions of Windows, Mac and Linux
platforms along with step-by-step installation instructions, see
\refappendix{install}.

\section{Building Stan}

Building Stan itself works the same way across platforms.
To build Stan, first open a command-line terminal application.  Then change
directories to the directory in which Stan is installed (i.e., the
directory containing the file named \code{makefile}).
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
% cd <stan-home>
\end{Verbatim}
\end{quote}
%
Then make the library with the folloiwng make command.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
% make bin/libstan.a
\end{Verbatim}
\end{quote}
%
and then make the model parser and code generator with
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
% make bin/stanc
\end{Verbatim}
\end{quote}
%
\emph{Warning}: The \code{make} program may take 10+ minutes and
consume 2+GB of memory to build \code{libstan} and \code{stanc}.
Compiler warnings, including \code{uname: not found}, may be safely ignored.

Building \code{libstan.a} and \code{bin/stanc} need only be done once.

\section{Compiling and Executing a Model}

The rest of this quick-start guide explains how to code
and run a very simple Bayesian model.

\subsection{A Simple Bernoulli Model}

The following simple model is available in the source
distribution located at \code{<stan-home>} as
%
\begin{quote}
\nolinkurl{src/models/basic_estimators/bernoulli.stan}
\end{quote}
%
The file contains the following model.
%
\begin{quote}
\begin{Verbatim}
data { 
  int<lower=0> N; 
  int<lower=0,upper=1> y[N];
} 
parameters {
  real<lower=0,upper=1> theta;
} 
model {
  theta ~ beta(1,1);
  for (n in 1:N) 
    y[n] ~ bernoulli(theta);
}
\end{Verbatim}
\end{quote}
%
The model assumes the binary observed data \code{y[1],...,y[N]}
are i.i.d.\ with Bernoulli chance-of-success \code{theta}.  The
prior on \code{theta} is \code{beta(1,1)} (i.e., uniform).%
%
\footnote{If no prior were specified in the model block, the
constraints on \code{theta} ensure it falls between 0 and 1,
providing \code{theta} an implicit uniform prior.  For parameters
with no prior specified and unbounded support, the result is an
improper prior.  Stan accepts improper priors, but posteriors must
be proper in order for sampling to succeed.}

\subsection{Data Set}

A data set of $\mbox{\code{N}}=10$ observations is availabe in the file
%
\begin{quote}
\nolinkurl{src/models/basic_estimators/bernoulli.Rdata}
\end{quote}
%
The content of the file is as follows.
%
\begin{quote}
\begin{Verbatim}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Verbatim}
\end{quote}
%
This defines the contents of two variables, \code{N} and \code{y},
using an R-like syntax (see \refchapter{dump} for more information).



\subsection{Generating and Compiling the Model}

A single call to \code{make} will generate the \Cpp code for a
model with a name ending in \code{.stan} and compile it for
execution.  This call will also compile the library \code{libstan.a}
and the parser/code generator \code{stanc} if they have not already
been compiled.

First, change directories to where Stan was unpacked.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd <stan-home>
\end{Verbatim}
\end{quote}
%
Then issue the following command.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> make src/models/basic_estimators/bernoulli 
\end{Verbatim}
\end{quote}
%%
%The \code{make} command may be issued from another directory as follows.
%%
%\begin{quote}
%\begin{Verbatim}[fontshape=sl]
%> make -f <stan-home>/makefile <path-to-model>
%\end{Verbatim}
%\end{quote}
%%
%where \code{<stan-home>} is replaced with the top-level directory
%containing the file named \code{makefile} and \code{<path-to-model>}
%is replaced with the path to the model file.

The C++ generated for the model and its compiled executable
form will be placed in the same directory as the model.


\subsection{Executing the Model}

The model can be executed from the directory in which it resides.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd src/models/basic_estimators 
\end{Verbatim}
\end{quote}
%
To execute the model under Linux or Mac, use
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli --data=bernoulli.Rdata
\end{Verbatim}
\end{quote}
%
The \code{./} prefix before the executable is only required under
Linux and the Mac when executing a model from the directory in which
it resides.

For the Windows DOS terminal, the \code{./} prefix is not needed,
resulting in the following command.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bernoulli --data=bernoulli.Rdata
\end{Verbatim}
\end{quote}
%
Whether the command is run in Windows, Linux, or on the Mac, the
output is the same.  First, the parameters are echoed to the standard output,
which shows up on the terminal as follows.
%
\begin{quote}
\begin{Verbatim}
STAN SAMPLING COMMAND
data = bernoulli.Rdata
init = random initialization
init tries = 1
samples = samples.csv
append_samples = 0
save_warmup = 0
seed = 1845979644 (randomly generated)
chain_id = 1 (default)
iter = 2000
warmup = 1000
thin = 1 (default)
equal_step_sizes = 0
leapfrog_steps = -1
max_treedepth = 10
epsilon = -1
epsilon_pm = 0
delta = 0.5
gamma = 0.05
...
\end{Verbatim}
\end{quote}
%
The ellipses (\code{...}) indicate that the output continues (as
described below).

Next, the sampler counts up the iterations in place, reporting
percentage completed, ending as follows.
%
\begin{quote}
\begin{Verbatim}
...
Iteration: 2000 / 2000 [100%]  (Sampling)
...
\end{Verbatim}
\end{quote}

\subsection{Sampler Output}

Each execution of the model results in the samples from a single
Markov chain being written to a file in comma-separated value (CSV) format.
The default name of the output file is \nolinkurl{samples.csv}.</p>

The first part of the output file just repeats the parameters
as comments (i.e., lines beginning with the pound sign (\Verb|#|)).
%
\begin{quote}
\begin{Verbatim}
...
# Samples Generated by Stan
#
# stan_version_major=1
# stan_version_minor=0
# stan_version_patch=0
# data=bernoulli.Rdata
# init=random initialization
# append_samples=0
# save_warmup=0
# seed=1845979644
# chain_id=1
# iter=2000
# warmup=1000
# thin=1
# equal_step_sizes=0
# leapfrog_steps=-1
# max_treedepth=10
# epsilon=-1
# epsilon_pm=0
# delta=0.5
# gamma=0.05
...
\end{Verbatim}
\end{quote}
%
This is then followed by a header indicating the
names of the values sampled.
%
\begin{quote}
\begin{Verbatim}
...
lp__,treedepth__,stepsize__,theta
...
\end{Verbatim}
\end{quote}
%
The first three names correspond to the log probability function
value at the sample, the depth of tree evaluated by the NUTS sampler,
and the step size.  The single model parameter \code{theta} is
stored in the fourth column.

Next up is the result of adaptation, reported as comments.
%
\begin{quote}
\begin{Verbatim}
...
# step size=1.43707
# parameter step size multipliers:
# 1
...
\end{Verbatim}
\end{quote}
%
This report says that NUTS step-size adaptation during warmup
settled on a step size of 1.43707.  The next two lines
indicate the multipliers for scaling individual parameters, here
just a single multiplier, 1, corresponding to the single
parameter \code{theta}.

The rest of the file contains lines corresponding to the
samples from each iteration.%
%
\footnote{There are repeated entries due to the Metropolis accept step
in the no-U-turn sampling algorithm.}
%
%
\begin{quote}
\begin{Verbatim}
...
-7.07769,1,1.43707,0.158674
-7.07769,1,1.43707,0.158674
-7.37289,1,1.43707,0.130089
-7.09254,1,1.43707,0.361906
-7.09254,1,1.43707,0.361906
-7.09254,1,1.43707,0.361906
-6.96213,1,1.43707,0.337061
-6.96213,1,1.43707,0.337061
-6.77689,1,1.43707,0.220795
...
-6.85235,1,1.43707,0.195994
-6.85235,1,1.43707,0.195994
-6.81491,1,1.43707,0.20624
-6.81491,1,1.43707,0.20624
-6.81491,1,1.43707,0.20624
-6.81491,1,1.43707,0.20624
-6.81491,1,1.43707,0.20624
\end{Verbatim}
\end{quote}
%
This ends the output.  

\subsection{Configuring Command-Line Options}

The command-line options for running a model are detailed in
the next chapter. They can also be printed on the command
line in Linux and on the Mac as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli --help
\end{Verbatim}
\end{quote}
%
and on Windows with
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bernoulli --help
\end{Verbatim}
\end{quote}

\subsection{Testing Stan}

To run the Stan unit tests of basic functionality, run the
following commands from a shell (where \code{<stan-home>} is replaced
top-level directory into which Stan was unpacked; it should contain a
file named \code{makefile}).
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd <stan-home>
> make O=0 test-unit
\end{Verbatim}
\end{quote}
%
That's the letter 'O' followed by an equal sign followed by the digit
'0', which sets the tests to run at the lowest optimization level.

\emph{Warning}: The \code{make} program may take 20+ minutes 
and consume 3+GB of memory to run the unit tests.  Warnings can
be safely ignored if the tests complete without a \code{FAIL} error.


% \section{User-Defined Distributions and Functions}

% \Stan allows new distributions to be coded directly in the modeling
% language.  

%  in one of two ways, either
% directly in its modeling language (see
% \refchapter{custom-probability-functions}), or by extending the
% modeling language using \Cpp (see
% \refappendix{user-defined-functions}).  The latter is more efficient
% and also portable across models.

